import os
import json
import asyncio
import aiohttp
import time
import traceback
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from dotenv import load_dotenv
from loguru import logger

# ---------------------------------------------------------------------------
#  ENV & LOGGER
# ---------------------------------------------------------------------------
load_dotenv()
lm_logger = logger.bind(channel="LM_STUDIO")


class LMStudioClient:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ OpenAI‚Äë—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–≥–æ API (LM¬†Studio).

    –ü–æ–∑–≤–æ–ª—è–µ—Ç:
    * –ø—Ä–æ–≤–µ—Ä—è—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ—Å—Ç–æ–≤
    * –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º/–ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    * –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å –∏ –∫—É—Å—Ç–∞—Ä–Ω–æ —Å—É–º–º–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ—Å—Ç–æ–≤ –≤ —Å—é–∂–µ—Ç—ã
    * –≤—ã–ø–æ–ª–Ω—è—Ç—å –±–∞–∑–æ–≤—ã–µ —Å–ª—É–∂–µ–±–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (`test_connection`, `get_models`)
    """

    # --------------------------- init -------------------------------------
    def __init__(self) -> None:
        # –ë–∞–∑–æ–≤—ã–π URL (–º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≤ .env)
        self.base_url: str = os.getenv("LM_STUDIO_URL", "http://localhost:1234/v1")

        # –ù–∞–∑–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π (—Ç–∞–∫–∂–µ —É–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —á–µ—Ä–µ–∑ .env)
        self.relevance_model: str = os.getenv("LM_STUDIO_RELEVANCE_MODEL", "local-model")
        self.classification_model: str = os.getenv("LM_STUDIO_CLASSIFICATION_MODEL", "local-model")
        self.analysis_model: str = os.getenv("LM_STUDIO_ANALYSIS_MODEL", "local-model")

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–µ—Ç–∏
        self.timeout: int = int(os.getenv("LM_STUDIO_TIMEOUT", "360"))
        self.max_retries: int = int(os.getenv("LM_STUDIO_MAX_RETRIES", "5"))

        # –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã
        self.relevance_temperature: float = float(os.getenv("LM_STUDIO_RELEVANCE_TEMP", "0.1"))
        self.classification_temperature: float = float(os.getenv("LM_STUDIO_CLASSIFICATION_TEMP", "0.1"))
        self.analysis_temperature: float = float(os.getenv("LM_STUDIO_ANALYSIS_TEMP", "0.3"))

        lm_logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω LM Studio –∫–ª–∏–µ–Ω—Ç: {self.base_url}")

    # --------------------------- Low‚Äëlevel HTTP ---------------------------
    async def _make_request(
        self,
        endpoint: str,
        payload: Dict[str, Any],
        retry_count: int = 0,
    ) -> Optional[Dict[str, Any]]:
        """–ù–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π POST‚Äë–∑–∞–ø—Ä–æ—Å —Å –ø—Ä–æ—Å—Ç–µ–π—à–∏–º back‚Äëoff."""
        start_time = time.perf_counter()
        lm_logger.debug(
            f"‚û°Ô∏è  POST {self.base_url}{endpoint} | payload={json.dumps(payload)[:300]}‚Ä¶"
        )

        url = f"{self.base_url}{endpoint}"
        timeout = aiohttp.ClientTimeout(total=self.timeout)

        try:
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(url, json=payload) as resp:
                    if resp.status == 200:
                        elapsed = time.perf_counter() - start_time
                        lm_logger.debug(
                            f"‚¨ÖÔ∏è  200 {endpoint} | {elapsed:.2f}s | size={resp.content_length or 'n/a'}"
                        )
                        return await resp.json()

                    # 5xx ‚ûú –ø–æ–≤—Ç–æ—Ä—è–µ–º —Å back‚Äëoff
                    err_text = await resp.text()
                    lm_logger.error(
                        f"‚ö†Ô∏è  {resp.status} {endpoint} | {err_text[:120]}‚Ä¶"
                    )
                    if resp.status in {500, 502, 503, 504} and retry_count < self.max_retries:
                        wait = 2 ** retry_count
                        lm_logger.info(f"–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ {wait}s‚Ä¶")
                        await asyncio.sleep(wait)
                        return await self._make_request(endpoint, payload, retry_count + 1)
                    return None

        except asyncio.TimeoutError:
            lm_logger.error(f"‚è±Ô∏è  –¢–∞–π–º–∞—É—Ç {self.timeout}s")
            if retry_count < self.max_retries:
                wait = 2 ** retry_count
                await asyncio.sleep(wait)
                return await self._make_request(endpoint, payload, retry_count + 1)
            return None
        except Exception as exc:  # noqa: BLE001
            lm_logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {exc}\n{traceback.format_exc()}")
            return None

    # --------------------------- Helpers ----------------------------------
    async def _chat_completion(
        self,
        prompt: str,
        *,
        temperature: float = 0.1,
        max_tokens: int = 256,
        model: str,
    ) -> Optional[Dict[str, Any]]:
        """–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ–±—ë—Ä—Ç–∫–∞ –Ω–∞–¥ /chat/completions."""
        payload = {
            "model": model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": temperature,
            "max_tokens": max_tokens,
        }
        return await self._make_request("/chat/completions", payload)

    @staticmethod
    def _extract_content(response: Dict[str, Any]) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç text –∏–∑ OpenAI‚Äë–ø–æ–¥–æ–±–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞."""
        try:
            return (
                response["choices"][0]["message"]["content"].strip()  # type: ignore[index]
            )
        except (KeyError, IndexError, TypeError):
            return None

    @staticmethod
    def _parse_json_response(response: Optional[Dict[str, Any]]) -> Optional[Any]:
        """–ü—ã—Ç–∞–µ—Ç—Å—è –∏–∑–≤–ª–µ—á—å –∏ —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON-–º–∞—Å—Å–∏–≤, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω –æ–±—ë—Ä–Ω—É—Ç –≤ Markdown-–±–ª–æ–∫–∏."""
        if response is None:
            return None
        content = LMStudioClient._extract_content(response)
        if not content:
            return None

        # –£–±–∏—Ä–∞–µ–º Markdown-–æ–±—ë—Ä—Ç–∫—É, –µ—Å–ª–∏ –µ—Å—Ç—å
        if content.strip().startswith("```json"):
            content = content.strip()
            content = content.removeprefix("```json").removesuffix("```").strip()

        try:
            return json.loads(content)
        except json.JSONDecodeError as e:
            lm_logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏ JSON: {e}")
            lm_logger.warning(f"–ù–µ—Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–π content:\n{content}")
            return None


    # --------------------------- Business API -----------------------------
    async def check_relevance(
        self, post_id: str, title: str, content: str
    ) -> Tuple[bool, float]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (relevant, score)."""
        lm_logger.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ {post_id}")

        if len(content) > 100_000:
            content = content[:100_000]

        prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –∏ –æ–ø—Ä–µ–¥–µ–ª–∏ –µ–≥–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Å–æ–≥–ª–∞—Å–Ω–æ —Å–ª–µ–¥—É—é—â–∏–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º.

–†–ï–õ–ï–í–ê–ù–¢–ù–´–ï –¢–ï–ú–´ (–¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É):

1. KYC/AML/Compliance:
   - KYC, Know Your Customer, "–∑–Ω–∞–π —Å–≤–æ–µ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞"
   - AML, Anti-Money Laundering, –ø—Ä–æ—Ç–∏–≤–æ–¥–µ–π—Å—Ç–≤–∏–µ –æ—Ç–º—ã–≤–∞–Ω–∏—é –¥–µ–Ω–µ–≥
   - Compliance, –∫–æ–º–ø–ª–∞–µ–Ω—Å, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º
   - –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–ª–∞–≥–æ–Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤
   - private wealth –∏–ª–∏ private management

2. –°–∞–Ω–∫—Ü–∏–∏ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∏:
   - –°–∞–Ω–∫—Ü–∏–æ–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏, OFAC, PEP (–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã–µ –ª–∏—Ü–∞)
   - World-Check, LexisNexis –∏ –¥—Ä—É–≥–∏–µ —Å–∏—Å—Ç–µ–º—ã –ø—Ä–æ–≤–µ—Ä–∫–∏
   - –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –∏–ª–∏ –∑–∞–∫—Ä—ã—Ç–∏–µ —Å—á–µ—Ç–æ–≤
   - –ü—Ä–æ–≤–µ—Ä–∫–∏ —á–∞—Å—Ç–Ω–æ–≥–æ –∫–∞–ø–∏—Ç–∞–ª–∞ (private wealth)

3. –†–µ–ø—É—Ç–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ä–∏—Å–∫–∏:
   - –†–µ–ø—É—Ç–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ä–∏—Å–∫–∏, —Ä–µ–ø—É—Ç–∞—Ü–∏–æ–Ω–Ω—ã–µ –∫—Ä–∏–∑–∏—Å—ã, —Ä–µ–ø—É—Ç–∞—Ü–∏–æ–Ω–Ω—ã–π —É—â–µ—Ä–± –¥–ª—è –∫–æ–º–ø–∞–Ω–∏–π
   - –û–Ω–ª–∞–π–Ω-—Ä–µ–ø—É—Ç–∞—Ü–∏—è, —Ü–∏—Ñ—Ä–æ–≤–∞—è —Ä–µ–ø—É—Ç–∞—Ü–∏—è
   - –ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è –∏–ª–∏ –ª–æ–∂–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤ –ø–æ–∏—Å–∫–æ–≤–æ–π –≤—ã–¥–∞—á–µ
   - –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –∏–ª–∏ —Ñ–µ–π–∫–æ–≤—ã–µ –æ—Ç–∑—ã–≤—ã –æ –±–∏–∑–Ω–µ—Å–µ
   - –ß–µ—Ä–Ω—ã–π PR, –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –∞—Ç–∞–∫–∏, PR-–∫—Ä–∏–∑–∏—Å—ã
   - –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–ø—É—Ç–∞—Ü–∏–µ–π, SERM, —Ü–∏—Ñ—Ä–æ–≤–æ–π –ø—Ä–æ—Ñ–∏–ª—å

4. –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç –ø–æ–∏—Å–∫–∞
   - –ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
   - –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
   - –ù–µ–π—Ä–æ—Å–µ—Ç–∏ –∏ –∏–Ω—Ç—Ä–µ–Ω–µ—Ç –ø–æ–∏—Å–∫
   - –ù–µ–π—Ä–æ—Å–µ—Ç–∏ –∏ —Ä–µ–ø—É—Ç–∞—Ü–∏–æ–Ω–Ω—ã–π –∫–æ–Ω—Å–∞–ª—Ç–∏–Ω–≥
   - –ê–ª–≥–æ—Ä–∏—Ç–º—ã Bing, Google, –Ø–Ω–¥–µ–∫—Å
   - PR, ORM, SEO, SERM –≤ —Ä–∞–±–æ—Ç–µ —Å —Ä–µ–ø—É—Ç–∞—Ü–∏–µ–π

–ò–°–ö–õ–Æ–ß–ï–ù–ò–Ø (–µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –ø—Ä–æ —ç—Ç–æ - –æ–Ω –ù–ï —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω):
- –°–ø–æ—Ä—Ç (—Ñ—É—Ç–±–æ–ª, —Ö–æ–∫–∫–µ–π, —Ç–µ–Ω–Ω–∏—Å –∏ —Ç.–¥.)
- –®–æ—É-–±–∏–∑–Ω–µ—Å, –∞—Ä—Ç–∏—Å—Ç—ã, –ø–µ–≤—Ü—ã, –∞–∫—Ç–µ—Ä—ã
- –†–∞–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç

–í–µ—Ä–Ω–∏ JSON —Å–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π:
{{
  "relevant": true/false,
  "score": 0.0-1.0,
  "reason": "–∫—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ",
  "matched_topics": ["—Å–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–µ–º"]
}}

–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}
–¢–µ–∫—Å—Ç: {content}"""

        resp = await self._chat_completion(
            prompt,
            temperature=self.relevance_temperature,
            max_tokens=512,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
            model=self.relevance_model,
        )
        parsed = self._parse_json_response(resp)
        if not parsed:
            return False, 0.0
        
        relevant = bool(parsed.get("relevant", False))
        score = float(parsed.get("score", 0.0))
        score = score if 0.0 <= score <= 1.0 else 0.0
        
        # –õ–æ–≥–∏—Ä—É–µ–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        if relevant:
            lm_logger.debug(
                f"–ü–æ—Å—Ç {post_id} —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω: score={score}, "
                f"topics={parsed.get('matched_topics', [])}, "
                f"reason={parsed.get('reason', 'N/A')}"
            )
        
        return relevant, score

    async def classify_content(
            self,
            post_id: str,
            title: str,
            content: str,
            categories: Dict[str, List[str]],
        ) -> Tuple[str, str, float]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (category, subcategory, confidence)."""
        from textwrap import dedent

        if len(content) > 100_000:
            content = content[:100_000]

        categories_str = "\n".join(
            f"{cat}: {', '.join(subs)}" for cat, subs in categories.items()
        )

        prompt = dedent(f"""
        –¢—ã –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—à—å –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –ø–æ —Å—Ç—Ä–æ–≥–æ –∑–∞–¥–∞–Ω–Ω–æ–π —Å—Ö–µ–º–µ.

        –£ —Ç–µ–±—è –µ—Å—Ç—å —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏ –∏—Ö –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π:

        {categories_str}

        –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –≤—ã–±—Ä–∞—Ç—å –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â—É—é **–∫–∞—Ç–µ–≥–æ—Ä–∏—é** –∏ **–ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—é** –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–π —Å—Ç–∞—Ç—å–∏, –∞ —Ç–∞–∫–∂–µ –æ—Ü–µ–Ω–∏—Ç—å —Å—Ç–µ–ø–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (–æ—Ç 0.0 –¥–æ 1.0).

        –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —Å–æ–±–ª—é–¥–∞–π —Å–ª–µ–¥—É—é—â–∏–µ –ø—Ä–∞–≤–∏–ª–∞:
        - –í—ã–±–∏—Ä–∞–π **—Ç–æ–ª—å–∫–æ –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π**.
        - –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π —Å–≤–æ–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.
        - –ï—Å–ª–∏ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç, –Ω–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è –ø–æ–¥—Ö–æ–¥–∏—Ç ‚Äî –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—é –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å –ø—É—Å—Ç–æ–π.
        - –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å **—Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON**:
        {{
            "category": "–ö–∞—Ç–µ–≥–æ—Ä–∏—è",
            "subcategory": "–ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è",
            "confidence": 0.87
        }}
        - **–ù–µ –æ—Å—Ç–∞–≤–ª—è–π category –ø—É—Å—Ç–æ–π**. –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ—à—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é ‚Äî –≤–µ—Ä–Ω–∏ "category": "–ü—Ä–æ—á–µ–µ" –∏ "subcategory": "".

        –í–æ—Ç —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏:

        –ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}

        –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {content}
        """)

        lm_logger.info(f"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è {post_id}")

        resp = await self._chat_completion(
            prompt,
            temperature=self.classification_temperature,
            model=self.classification_model,
        )

        parsed = self._parse_json_response(resp)
        lm_logger.debug(f"[LM RESPONSE] {post_id}: {parsed}")

        if not parsed:
            return "", "", 0.0

        cat = parsed.get("category", "").strip()
        sub = parsed.get("subcategory", "").strip()
        conf = parsed.get("confidence", 0.0)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫—É
        if cat not in categories:
            lm_logger.warning(f"[LM INVALID] {post_id} ‚Äî –Ω–µ–≤–∞–ª–∏–¥–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è: {cat}")
            return "", "", 0.0

        if sub and sub not in categories[cat]:
            lm_logger.warning(f"[LM SUB] {post_id} ‚Äî –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è '{sub}' –Ω–µ –≤ —Å–ø–∏—Å–∫–µ –¥–ª—è '{cat}'")
            sub = ""  # –ø—Ä–∏–Ω–∏–º–∞–µ–º –ø—É—Å—Ç—É—é –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—é

        conf = conf if 0.0 <= conf <= 1.0 else 0.0
        return cat, sub, conf


    async def analyze_and_summarize(self, posts: list[dict], max_stories: int = 10) -> list:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Å—Ç–∞ –æ—Ç–¥–µ–ª—å–Ω–æ (–ø–æ –æ–¥–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É).

        Args:
            posts: —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∫–ª—é—á–∞–º–∏: post_id, title, content, url (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            max_stories: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—Ä–∏–π

        Returns:
            —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∫–ª—é—á–∞–º–∏: post_id, title, summary
        """
        if not posts:
            lm_logger.warning("–ù–µ—Ç –ø–æ—Å—Ç–æ–≤ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏")
            return []

        summaries = []

        for i, post in enumerate(posts[:max_stories], 1):
            post_id = post.get("post_id", "").strip()
            title = post.get("title", "").strip()
            content = post.get("content", "").strip()

            if not post_id or not content:
                continue

            prompt = f"""
    –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç—ã –Ω–∏–∂–µ –∏ —Å–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–∏–µ —Å–∞–º–º–∞—Ä–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.

    –ü–†–ò–ú–ï–† –ü–†–ê–í–ò–õ–¨–ù–û–ì–û –û–¢–í–ï–¢–ê:
    [
    {{
        "post_id": "{post_id}",
        "title": "–ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—å–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ",
        "summary": "–ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –û—Å–Ω–æ–≤–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã –∏ –≤—ã–≤–æ–¥—ã –≤ 5-7 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π."
    }}
    ]

    –ò–ù–°–¢–†–£–ö–¶–ò–Ø:
    1. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å–æ–∑–¥–∞–π –æ–±—ä–µ–∫—Ç —Å –ø–æ–ª—è–º–∏: post_id, title, summary
    2. –í –ø–æ–ª–µ post_id –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —Å–∫–æ–ø–∏—Ä—É–π –¢–û–ß–ù–û–ï –∑–Ω–∞—á–µ–Ω–∏–µ ID –∏–∑ —Ç–µ–∫—Å—Ç–∞ –Ω–∏–∂–µ
    3. –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Å–∞–º–º–∞—Ä–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
    4. –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON –º–∞—Å—Å–∏–≤

    –¢–ï–ö–°–¢–´ –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê:
    ============================================================
    –¢–µ–∫—Å—Ç ‚Ññ{i}
    ID: {post_id}
    –ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}
    –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {content[:5000]}
    ============================================================

    –°–æ–∑–¥–∞–π JSON –º–∞—Å—Å–∏–≤ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ –≤—ã—à–µ:
    """

            lm_logger.info(f"üìÑ –ê–Ω–∞–ª–∏–∑ –ø–æ—Å—Ç–∞ {i}/{max_stories} ‚Äî ID: {post_id}")
            resp = await self._chat_completion(
                prompt,
                temperature=self.analysis_temperature,
                max_tokens=1024,
                model=self.analysis_model,
            )

            parsed = self._parse_json_response(resp)
            if isinstance(parsed, list) and parsed and "summary" in parsed[0]:
                summaries.append(parsed[0])
                lm_logger.info(f"‚úÖ –ü–æ—Å—Ç {post_id} –æ–±—Ä–∞–±–æ—Ç–∞–Ω")
            elif isinstance(parsed, dict) and "summary" in parsed:
                summaries.append(parsed)
                lm_logger.info(f"‚úÖ –ü–æ—Å—Ç {post_id} –æ–±—Ä–∞–±–æ—Ç–∞–Ω (dict)")
            else:
                lm_logger.warning(f"‚ùå –û—Ç–≤–µ—Ç –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω –¥–ª—è post_id: {post_id}")

        return summaries


    # --------------------------- Service API ------------------------------
    async def test_connection(self) -> bool:
        """–ë—ã—Å—Ç—Ä—ã–π –ø–∏–Ω–≥‚Äëcheck."""
        resp = await self._chat_completion(
            "ping",
            temperature=0.0,
            model=self.relevance_model,
            max_tokens=5,
        )
        ok = bool(resp and "choices" in resp)
        lm_logger.info("LM Studio API –¥–æ—Å—Ç—É–ø–µ–Ω" if ok else "LM Studio API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        return ok

    async def get_models(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π."""
        url = f"{self.base_url}/models"
        timeout = aiohttp.ClientTimeout(total=10)
        try:
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(url) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        return [m["id"] for m in data.get("data", [])]
                    lm_logger.error(f"–û—à–∏–±–∫–∞ {resp.status} –ø—Ä–∏ GET /models")
        except Exception as exc:  # noqa: BLE001
            lm_logger.error(f"get_models error: {exc}")
        return []

    # --------------------------- top relevance ------------------------------
    async def select_top_posts(self, posts: list[dict], top_n: int = 5) -> list[dict]:
        """
        –û—Ç–±–æ—Ä –¥–æ 5 –Ω–∞–∏–±–æ–ª–µ–µ –∑–Ω–∞—á–∏–º—ã—Ö –∏ –Ω–µ–ø–æ—Ö–æ–∂–∏—Ö –¥—Ä—É–≥ –Ω–∞ –¥—Ä—É–≥–∞ –ø–æ—Å—Ç–æ–≤ —Å—Ä–µ–¥–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö.

        –≠—Ç–∞–ø—ã:
        1. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (–≤ —Ç–æ–º —á–∏—Å–ª–µ –ø–æ —Å–º—ã—Å–ª—É)
        2. –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (—Å—Ç—Ä–æ–≥–∞—è)
        3. –í—ã–±–æ—Ä –Ω–∞–∏–±–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤
        """
        if not posts:
            lm_logger.warning("–ù–µ—Ç –ø–æ—Å—Ç–æ–≤ –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
            return []

        # –®–∞–≥ 1. –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Å–º—ã—Å–ª—É
        contents = [p["content"] for p in posts]
        vectorizer = TfidfVectorizer().fit_transform(contents)
        similarity_matrix = cosine_similarity(vectorizer)
        np.fill_diagonal(similarity_matrix, 0)

        unique_indices = []
        for i, row in enumerate(similarity_matrix):
            if all(similarity_matrix[i, j] < 0.9 for j in unique_indices):
                unique_indices.append(i)

        unique_posts = [posts[i] for i in unique_indices]
        lm_logger.info(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ —Å–º—ã—Å–ª—É: {len(unique_posts)}")

        # –®–∞–≥ 2. –ü–æ–≤—Ç–æ—Ä–Ω–∞—è —Å—Ç—Ä–æ–≥–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        rechecked = []
        for post in unique_posts:
            try:
                relevant, score = await self.check_relevance(
                    post_id=post["post_id"],
                    title=post["title"],
                    content=post["content"],
                )
                if relevant:
                    post["score"] = score
                    rechecked.append(post)
            except Exception as e:
                lm_logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: {e}")

        if not rechecked:
            lm_logger.warning("–ù–µ—Ç –ø–æ—Å—Ç–æ–≤, –ø—Ä–æ—à–µ–¥—à–∏—Ö –ø–æ–≤—Ç–æ—Ä–Ω—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å")
            return []

        # –®–∞–≥ 3. –û—Ç–±–æ—Ä –Ω–∞–∏–±–æ–ª–µ–µ –Ω–µ–ø–æ—Ö–æ–∂–∏—Ö (diverse) –ø–æ—Å—Ç–æ–≤ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º score
        rechecked.sort(key=lambda x: x["score"], reverse=True)
        selected = []
        selected_vectors = []

        tfidf = TfidfVectorizer().fit([p["content"] for p in rechecked])
        for post in rechecked:
            vec = tfidf.transform([post["content"]])
            if all(cosine_similarity(vec, v)[0][0] < 0.8 for v in selected_vectors):
                selected.append(post)
                selected_vectors.append(vec)
            if len(selected) >= top_n:
                break

        lm_logger.info(f"–§–∏–Ω–∞–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–æ {len(selected)} –ø–æ—Å—Ç–æ–≤ –¥–ª—è Telegram")
        return selected

     # --------------------------- new relevance ------------------------------
    async def recheck_relevance_strict(self, posts: list[dict]) -> list[dict]:
        """
        –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Å –±–æ–ª–µ–µ –∂—ë—Å—Ç–∫–∏–º–∏ –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Ç–µ –ø–æ—Å—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ—à–ª–∏ –ø–æ—Ä–æ–≥.
        """
        lm_logger.info(f"–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: {len(posts)} –ø–æ—Å—Ç–æ–≤")
        filtered = []

        for i, post in enumerate(posts, 1):
            post_id = post.get("post_id", "")
            title = post.get("title", "")
            content = post.get("content", "")
            if not content:
                continue

            prompt = f"""
    –û—Ü–µ–Ω–∏ —Å—Ç—Ä–æ–≥–æ, —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω –ª–∏ —Ç–µ–∫—Å—Ç —Å–ª–µ–¥—É—é—â–∏–º —Ç–µ–º–∞–º:

    1. KYC/AML/Compliance
    2. –°–∞–Ω–∫—Ü–∏–∏ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∏
    3. –†–µ–ø—É—Ç–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ä–∏—Å–∫–∏
    4. –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–ø–æ–∏—Å–∫–∞

    –ò–°–ö–õ–Æ–ß–ï–ù–ò–Ø:
    - —Å–ø–æ—Ä—Ç, —à–æ—É-–±–∏–∑–Ω–µ—Å, —Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏—è

    –û—Ç–≤–µ—Ç –≤ JSON:
    {{ "relevant": true/false, "score": float, "reason": str }}

    –ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}
    –¢–µ–∫—Å—Ç: {content[:3000]}
    """

            lm_logger.info(f"üîÅ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: {post_id} ({i}/{len(posts)})")
            resp = await self._chat_completion(
                prompt,
                temperature=self.relevance_temperature,
                max_tokens=512,
                model=self.relevance_model,
            )
            parsed = self._parse_json_response(resp)
            if parsed and parsed.get("relevant") and float(parsed.get("score", 0)) >= 0.7:
                filtered.append(post)

        lm_logger.info(f"‚úÖ –ü–æ–≤—Ç–æ—Ä–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö: {len(filtered)} –∏–∑ {len(posts)}")
        return filtered
